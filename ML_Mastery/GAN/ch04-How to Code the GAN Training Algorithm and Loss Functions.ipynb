{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch04-How to Code the GAN Training Algorithm and Loss Functions.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPt3xuaFiOkjDWEEwFbr0iz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#ch04-How to Code the GAN Training Algorithm and Loss Functions\n","- [How to Code the GAN Training Algorithm and Loss Functions](https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/), by [Jason Brownlee](https://machinelearningmastery.com/author/jasonb/) on January 10, 2020 in [Generative Adversarial Networks](https://machinelearningmastery.com/category/generative-adversarial-networks/)\n","\n","- revised by Ivan HP Lin (220506)"],"metadata":{"id":"YE-yLBEO6dRc"}},{"cell_type":"markdown","source":["The Generative Adversarial Network, or GAN for short, is an architecture for training a generative model.\n","\n","The architecture is comprised of two models. The generator that we are interested in, and a discriminator model that is used to assist in the training of the generator. \n","\n","It can be challenging to understand how a GAN is trained and exactly how to understand and implement the loss function for the generator and discriminator models.\n","\n","After completing this tutorial, you will know:\n","\n","- How to Implement the **GAN Training Algorithm**\n","- Understanding the **GAN Loss Function**\n","- How to **Train GAN Models** in Practice, and how to implement **weight updates** for the discriminator and generator models in practice"],"metadata":{"id":"rV4gJGb9EHhJ"}},{"cell_type":"markdown","source":["## 1.. How to Implement the GAN Training Algorithm"],"metadata":{"id":"cTVZ5ei2FmF5"}},{"cell_type":"markdown","source":["The GAN training algorithm involves training both the discriminator and the generator model in parallel.\n","\n","The algorithm is summarized in the figure below, taken from the original 2014 paper by Goodfellow, et al. titled “[**Generative Adversarial Networks**](https://arxiv.org/abs/1406.2661)”\n","\n","<figure>\n","<center>\n","<img src=\"https://machinelearningmastery.com/wp-content/uploads/2019/05/Summary-of-the-Generative-Adversarial-Network-Training-Algorithm.png\" width=\"80%\">\n","<figcaption>Summary of the Generative Adversarial Network Training Algorithm.Taken from: Generative Adversarial Networks.</figcaption></center>\n","</figure>"],"metadata":{"id":"TAuXKJ7RFtaG"}},{"cell_type":"markdown","source":["Let’s take some time to unpack and get comfortable with this algorithm.\n","\n","The outer loop of the algorithm involves iterating over steps to train the models in the architecture. One cycle through this loop is not an epoch: it is a single update comprised of specific batch updates to the discriminator and generator models.\n","\n","An **[epoch](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)** is defined as one cycle through a training dataset, where the samples in a training dataset are used to update the model weights in mini-batches. For example, a training dataset of 100 samples used to train a model with a mini-batch size of 10 samples would involve 10 mini batch updates per epoch. The model would be fit for a given number of epochs, such as 500.\n","\n","This is often hidden from you via the automated training of a model via a call to the **fit()** function and specifying the number of epochs and the size of each mini-batch.\n","\n","In the case of the GAN, the number of training iterations must be defined based on the size of your training dataset and batch size. In the case of a dataset with 100 samples, a batch size of 10, and 500 training epochs, we would first calculate the number of batches per epoch and use this to calculate the total number of training iterations using the number of epochs.  For example:\n","```\n","batches_per_epoch = floor(dataset_size / batch_size)\n","total_iterations = batches_per_epoch * total_epochs\n","```\n","In the case of a dataset of 100 samples, a batch size of 10, and 500 epochs, the GAN would be trained for $floor({ {100}\\over{10}}) * 500$ or 5,000 total iterations."],"metadata":{"id":"O3goUU-GIjoB"}},{"cell_type":"markdown","source":["Next, we can see that one iteration of training results in \n","* possibly multiple updates to the **discriminator** and \n","* one update to the **generator**, \n","  - number of updates to the discriminator is a hyperparameter that is set to 1.\n","\n","- The training process consists of **simultaneous SGD**. **On each step, two minibatches are sampled**: \n","  1. a minibatch of x values from the dataset and \n","  2. a minibatch of z values drawn from the model’s prior over latent variables. \n","  \n","  Then two gradient steps are made simultaneously …\n","\n","  — [NIPS 2016 Tutorial: Generative Adversarial Networks, 2016](https://arxiv.org/abs/1701.00160).\n","\n","We can therefore summarize the training algorithm with Python pseudocode as follows:\n","```\n","# gan training algorithm\n","def train_gan(dataset, n_epochs, n_batch):\n","\t# calculate the number of batches per epoch\n","\tbatches_per_epoch = int(len(dataset) / n_batch)\n","\t\n","  # calculate the number of training iterations\n","\tn_steps = batches_per_epoch * n_epochs\n","\t\n","  #######################\n","  # gan training algorithm\n","\tfor i in range(n_steps):\n","\t\t# update the discriminator model\n","\t\t# ...\n","\t\t# update the generator model\n","\t\t# ...\n","```"],"metadata":{"id":"gjftw-9K3B0X"}},{"cell_type":"markdown","source":["### 1.1 Updating **Discriminator** model"],"metadata":{"id":"tYoPaaH05m79"}},{"cell_type":"markdown","source":["Updating the discriminator model involves a few steps.\n","\n","1. First, a batch of random points from the latent space must be selected for use as input to the generator model to provide the basis for the generated or ‘fake‘ samples. \n","  - Then a batch of samples from the training dataset must be selected for input to the discriminator as the ‘real‘ samples.\n","\n","2. Next, the discriminator model must make predictions for the real and fake samples and the weights of the discriminator must be updated proportional to how correct or incorrect those predictions were. \n","  - The predictions are probabilities and we will get into the nature of the predictions and the loss function in the next section. For now, we can outline what these steps actually look like in practice."],"metadata":{"id":"jBkLgkPa4eW2"}},{"cell_type":"markdown","source":["Next, we must generate points from the latent space and then use the generator model in its current form to generate some fake images. For example:\n","```\n","...\n","# generate points in the latent space\n","z = randn(latent_dim * n_batch)\n","\n","# reshape into a batch of inputs for the network\n","z = x_input.reshape(n_batch, latent_dim)\n","\n","# generate fake images\n","fake = generator.predict(z)\n","```\n","\n","Note that the size of the latent dimension is also provided as a hyperparameter to the training algorithm."],"metadata":{"id":"bCRlEJ8x5y_s"}},{"cell_type":"markdown","source":["We then must select a batch of real samples, and this too will be wrapped into a function.\n","```\n","# select a batch of random real images\n","ix = randint(0, len(dataset), n_batch)\n","# retrieve real images\n","real = dataset[ix]\n","```"],"metadata":{"id":"Y1a7agcO7Bdy"}},{"cell_type":"markdown","source":["The discriminator model must then make a prediction for each of the generated and real images and the weights must be updated.\n","```\n","# gan training algorithm\n","def train_gan(generator, discriminator, dataset, latent_dim, n_epochs, n_batch):\n","\t# calculate the number of batches per epoch\n","\tbatches_per_epoch = int(len(dataset) / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = batches_per_epoch * n_epochs\n","\t\n","  # gan training algorithm\n","\tfor i in range(n_steps):\n","\t\t# generate points in the latent space\n","\t\tz = randn(latent_dim * n_batch)\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz = z.reshape(n_batch, latent_dim)\n","\t\t# generate fake images\n","\t\tfake = generator.predict(z)\n","\t\t# select a batch of random real images\n","\t\tix = randint(0, len(dataset), n_batch)\n","\t\t# retrieve real images\n","\t\treal = dataset[ix]\n","\t\t# update weights of the discriminator model\n","\t\t# ...\n","\n","\t\t# update the generator model\n","\t\t# ...\n","```"],"metadata":{"id":"zQ4bVGc97-MK"}},{"cell_type":"markdown","source":["### 1.2 Updating **Generator** model"],"metadata":{"id":"iT9t8M7g0LIf"}},{"cell_type":"markdown","source":["Next, the generator model must be updated.\n","\n","Again, a batch of random points from the latent space must be selected and passed to the generator to generate fake images, and then passed to the discriminator to classify.\n","```\n","...\n","# generate points in the latent space\n","z = randn(latent_dim * n_batch)\n","# reshape into a batch of inputs for the network\n","z = z.reshape(n_batch, latent_dim)\n","# generate fake images\n","fake = generator.predict(z)\n","# classify as real or fake\n","result = discriminator.predict(fake)\n","```"],"metadata":{"id":"OUv4r-SZscBQ"}},{"cell_type":"markdown","source":["The response can then be used to update the weights of the generator model.\n","\n","```\n","# gan training algorithm\n","def train_gan(generator, discriminator, dataset, latent_dim, n_epochs, n_batch):\n","\t# calculate the number of batches per epoch\n","\tbatches_per_epoch = int(len(dataset) / n_batch)\n","\t# calculate the number of training iterations\n","\tn_steps = batches_per_epoch * n_epochs\n","\n","\t# gan training algorithm\n","\tfor i in range(n_steps):\n","\t\t# generate points in the latent space\n","\t\tz = randn(latent_dim * n_batch)\n","\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz = z.reshape(n_batch, latent_dim)\n","\n","\t\t# generate fake images\n","\t\tfake = generator.predict(z)\n","\n","\t\t# select a batch of random real images\n","\t\tix = randint(0, len(dataset), n_batch)\n","\n","\t\t# retrieve real images\n","\t\treal = dataset[ix]\n","\n","\t\t# update weights of the discriminator model\n","\t\t# ...\n","\t\t# generate points in the latent space\n","\t\tz = randn(latent_dim * n_batch)\n","\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz = z.reshape(n_batch, latent_dim)\n","\n","\t\t# generate fake images\n","\t\tfake = generator.predict(z)\n","\n","\t\t# classify as real or fake\n","\t\tresult = discriminator.predict(fake)\n","\n","\t\t# update weights of the generator model\n","\t\t# ...\n","```\n","\n","It is interesting that the discriminator is updated with two batches of samples each training iteration whereas the generator is only updated with a single batch of samples per training iteration.\n","Now that we have defined the training algorithm for the GAN, we need to understand how the model weights are updated. This requires understanding the loss function used to train the GAN."],"metadata":{"id":"BqwjafWusqz4"}},{"cell_type":"markdown","source":["## 2.. Understanding the GAN Loss Function"],"metadata":{"id":"v8Ai9KEdtpGw"}},{"cell_type":"markdown","source":["### **Discriminator**\n","\n","* The discriminator is trained to correctly classify real and fake images.\n","\n","* This is achieved by **maximizing the log of predicted probability of real images and the log of the inverted probability of fake images, averaged over each mini-batch of examples**.\n","\n","\n","Recall that we add log probabilities, which is the same as multiplying probabilities, although without vanishing into small numbers. Therefore, we can understand this loss function as seeking probabilities close to 1.0 for real images and probabilities close to 0.0 for fake images, inverted to become larger numbers. The addition of these values means that lower average values of this loss function result in better performance of the discriminator.\n","\n","Inverting this to a minimization problem, it should not be surprising if you are familiar with developing neural networks for binary classification, as this is exactly the approach used.\n","\n","- This is just the standard cross-entropy cost that is minimized when training a standard binary classifier with a sigmoid output. The only difference is that the classifier is trained on two minibatches of data; \n","  - one coming from the dataset, where the label is 1 for all examples, \n","  - and one coming from the generator, where the label is 0 for all examples.\n","  - [NIPS 2016 Tutorial: Generative Adversarial Networks, 2016](https://arxiv.org/abs/1701.00160)."],"metadata":{"id":"PdjoFb-XuM33"}},{"cell_type":"markdown","source":["### **Generator**\n","\n","The generator is more tricky.\n","\n","The GAN algorithm defines the generator model’s loss as *minimizing the log of the inverted probability of the discriminator’s prediction of fake images, averaged over a mini-batch*.\n","\n","This is straightforward, but according to the authors, it is not effective in practice when the generator is poor and the discriminator is good at rejecting fake images with high confidence. The loss function no longer gives good gradient information that the generator can use to adjust weights and instead saturates.\n","\n","- In this case, $log(1 − D(G(z)))$ saturates. Rather than training $G$ to minimize $log(1 − D(G(z)))$ we can train $G$ to maximize $log D(G(z))$. This objective function results in the same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.\n","\n","  - [Generative Adversarial Networks, 2014](https://arxiv.org/abs/1406.2661)\n","\n","Instead, the authors recommend maximizing the log of the discriminator’s predicted probability for fake images. \n","The change is subtle.\n","\n","* In the first case, the generator is trained to minimize the probability of the discriminator being correct. With this change to the loss function, the generator is trained to maximize the probability of the discriminator being incorrect.\n","\n","  - In the minimax game, the generator minimizes the log-probability of the discriminator being correct. In this game, the generator maximizes the log probability of the discriminator being mistaken.\n","  - [NIPS 2016 Tutorial: Generative Adversarial Networks, 2016](https://arxiv.org/abs/1701.00160)\n","\n","The sign of this loss function can then be inverted to give a familiar minimizing loss function for training the generator. As such, this is sometimes referred to as the $-log D$ trick for training GANs.\n","\n","  Our baseline comparison is DCGAN, a GAN with a convolutional architecture trained with the standard GAN procedure using the −log D trick.\n","  - [Wasserstein GAN, 2017](https://arxiv.org/abs/1701.07875)\n","\n","Now that we understand the GAN loss function, we can look at how the discriminator and the generator model can be updated in practice."],"metadata":{"id":"h5W8rfAavjlP"}},{"cell_type":"markdown","source":["## 3.. How to Train GAN Models in Practice"],"metadata":{"id":"4iKMGOrmz25X"}},{"cell_type":"markdown","source":["### **Discriminator** $(D)$"],"metadata":{"id":"rJISHfPH24KV"}},{"cell_type":"markdown","source":["The practical implementation of the GAN loss function and model updates is straightforward.\n","\n","We can implement the **discriminator ($D$)** directly by configuring \n","* Minimizing the **cross-entropy loss**, specifically the binary cross-entropy loss.\n","  - probability of 1 for the discriminator model to predict real images \n","  - probability of 0 for the discriminator model to predict fake images\n","\n","\n","For example, a snippet of our model definition with Keras for the discriminator might look as follows for the output layer and the compilation of the model with the appropriate loss function.\n","```\n","# output layer\n","model.add(Dense(1, activation='sigmoid'))\n","# compile model\n","model.compile(loss='binary_crossentropy', ...)\n","```"],"metadata":{"id":"W0oazqJf0ZjH"}},{"cell_type":"markdown","source":["The defined model can be trained for each batch of real and fake samples providing arrays of $1's$ and $0's$ for the expected outcome.\n","\n","The [ones()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html) and [zeros()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) NumPy functions can be used to create these target labels, and the Keras function **train_on_batch()** can be used to update the model for each batch of samples.\n","\n","```\n","...\n","X_fake = ...\n","X_real = ...\n","# define target labels for fake images\n","y_fake = zeros((n_batch, 1))\n","\n","# update the discriminator for fake images\n","discriminator.train_on_batch(X_fake, y_fake)\n","\n","# define target labels for real images\n","y_real = ones((n_batch, 1))\n","\n","# update the discriminator for real images\n","discriminator.train_on_batch(X_real, y_real)\n","```\n","\n","The discriminator model will be trained to predict the probability of “realness” of a given input image that can be interpreted as a class label of class=0 for fake and class=1 for real."],"metadata":{"id":"pAY-8ccA2BAu"}},{"cell_type":"markdown","source":["### Generator $(G)$\n","\n","The generator is trained to maximize the discriminator predicting a high probability of “realness” for generated images.\n","\n","This is achieved **by updating the generator via the discriminator with the class label of 1 for the generated images**. The discriminator is not updated in this operation but provides the gradient information required to update the weights of the generator model.\n","\n","For example, if the discriminator predicts a low average probability for the batch of generated images, then this will result in a large error signal propagated backward into the generator given the “expected probability” for the samples was 1.0 for real. This large error signal, in turn, results in relatively large changes to the generator to hopefully improve its ability at generating fake samples on the next batch.\n","\n","This can be implemented in Keras by creating a composite model that combines the generator and discriminator models, \n","* allowing the output images from the generator to flow into discriminator directly, and in turn, \n","* allow the error signals from the predicted probabilities of the discriminator to flow back through the weights of the generator model.\n","```\n","# define a composite gan model for the generator and discriminator\n","def define_gan(generator, discriminator):\n","\t# make weights in the discriminator not trainable\n","\tdiscriminator.trainable = False\n","\t\n","  # connect them\n","\tmodel = Sequential()\n","\t\n","  # add generator\n","\tmodel.add(generator)\n","\t\n","  # add the discriminator\n","\tmodel.add(discriminator)\n","\t\n","  # compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n","\treturn model\n","```\n","\n","The composite model can then be updated using fake images and real class labels.\n","```\n","...\n","# generate points in the latent space\n","z = randn(latent_dim * n_batch)\n","\n","# reshape into a batch of inputs for the network\n","z = z.reshape(n_batch, latent_dim)\n","\n","# define target labels for real images\n","y_real = ones((n_batch, 1))\n","\n","# update generator model\n","gan_model.train_on_batch(z, y_real)\n","```\n","\n","That completes out tour of the GAN training algorithm, loss function and weight update details for the discriminator and generator models."],"metadata":{"id":"EEg-ZNr82_G0"}},{"cell_type":"markdown","source":["## 4.. Further Readings\n","\n","Papers\n","- [Generative Adversarial Networks, 2014](https://arxiv.org/abs/1406.2661)\n","- [NIPS 2016 Tutorial: Generative Adversarial Networks, 2016](https://arxiv.org/abs/1701.00160)\n","- [Wasserstein GAN, 2017](https://arxiv.org/abs/1701.07875)\n","\n","Articles\n","- [**Understanding Generative Adversarial Networks, 2017**](https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/)."],"metadata":{"id":"k1sxfYDmaVtF"}}]}