{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Hands-On RL Courses_From zero to hero, step by step.ipynb","provenance":[],"authorship_tag":"ABX9TyMSmSdxLse3VnjLWPNmXqRn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[Hands-On Reinforcement Learning Course: Part 1 - From zero to hero, step by step](https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-1-269b50e39d08)\n","\n","by [Pau Labarta Bajo](https://pau-labarta-bajo.medium.com/?source=post_page-----269b50e39d08--------------------------------), Nov 27, 2021.\n","\n","Summarized by Ivan H.P. Lin"],"metadata":{"id":"Mi3Gntqqhw_t"}},{"cell_type":"markdown","source":["#0 - Contents\n","1. What is a Reinforcement Learning problem? ü§î\n","2. Policies üëÆüèΩ and value functions.\n","3. How to generate the training data? üìä\n","4. Python boilerplate code.üêç\n","5. Recap ‚ú®\n","6. Homework üìö\n","7. What‚Äôs next? ‚ù§Ô∏è\n","\n","Let‚Äôs start!"],"metadata":{"id":"krYlwHGuheqF"}},{"cell_type":"markdown","source":["#1 - What is a reinforcement learning problem? ü§î\n","**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**"],"metadata":{"id":"0yxeIlc4iPq1"}},{"cell_type":"markdown","source":["An intelligent **agent** ü§ñ needs to learn, through trial and error, how to take **actions** inside and **environment** üåé in order to maximize a **cumulative reward**.\n","\n","If you are asking yourself these questions you are on the right track.\n","\n","The definition I just gave introduces a bunch of terms that you might not be familiar with. In fact, they are ambiguous on purpose. This generality is what makes RL applicable to a wide range of seemingly different learning problems. This is the philosophy behind mathematical modeling, which stays at the roots of RL.\n","\n","Let's use few examples to explain **AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**:"],"metadata":{"id":"VKMFBfasif_t"}},{"cell_type":"markdown","source":["##**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","### Example 1: learning to walk üö∂üèΩ‚Äç‚ôÄÔ∏èüö∂üèøüö∂‚Äç‚ôÄÔ∏è"],"metadata":{"id":"aUyIV7F2VE5s"}},{"cell_type":"markdown","source":["### **AGENT**\n","* The **agent** is my son, Kai. And he wants to stand up and walk. His muscles are strong enough at this point in time to have a chance at it. The learning problem for him is: how to sequentially adjust his body position, including several angles on his legs, waist, back, and arms to balance his body and not fall.\n"],"metadata":{"id":"WH9gDsyCjD32"}},{"cell_type":"markdown","source":["### **ENVIRONMENT**\n","* The **environment** is the physical world surrounding him, including the laws of physics. The most important of which is gravity. Without gravity the learning-to-walk problem would drastically change, and even become irrelevant: why would you wanna walk in a world where you can simply fly? Another important law in this learning problem is Newton‚Äôs third law, which in plain words tells that if you fall on the floor, the floor is going to hit you back with the same strength. Ouch!"],"metadata":{"id":"LCc4f4_jjdZ1"}},{"cell_type":"markdown","source":["### **ACTION**\n","* The **actions** are all the updates in these body angles, which determine his body position and speed as he starts chasing things around. \n","  - Sure he can do other things at the same time, like imitating the sound of a cow, but these are probably not helping him accomplish his goal. We ignore these actions in our framework. Adding unnecessary actions does not change the modeing step, but it makes the problem harder to solve later on."],"metadata":{"id":"_uLvFVjDSVSU"}},{"cell_type":"markdown","source":["### **REWARDS**, and ***Cumulative Reward***\n","* The **reward** he receives is a stimulus coming from the brain, that makes him happy or makes him feel pain. There is the negative reward he experiences when falling on the floor, which is physical pain maybe followed by frustration. \n","  - On the other side, there are several things that contribute positively to his happiness, like the happiness of getting to places faster üë∂üöÄ, or the external stimulus that comes from my wife Jagoda and I when we say ‚ÄúGood job!‚Äù or ‚ÄúBravo!‚Äù to each attempt and marginal improvement he shows."],"metadata":{"id":"8RLPytNVSgZr"}},{"cell_type":"markdown","source":["* A little bit more about rewards üí∞\n","Some actions might seem very appealing for the baby at the beginning, like trying to run to get a boost of excitement. However, he soon learns that in some (or most) cases he ends up falling on his face, and experiencing an extended period of pain and tears. This is why **intelligent agents maximize cumulative reward**, and **not marginal reward**. \n","  * **They trade short-term rewards with long-term ones. An action that would give immediate reward, but put my body in a position about to fall, is not an optimal one.**\n","\n","* The frequency and intensity of the rewards are key for helping the agent learn. Very infrequent (sparse) feedback means harder learning. Think about it, if you do not know if what you do is good or bad, how can you learn? This is one of the main reasons why some RL problems are harder than others.\n","\n","* Reward shaping is a tough modeling decision for many real-world RL problems.\n"],"metadata":{"id":"dwc6dmA9SigL"}},{"cell_type":"markdown","source":["##**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","### Example 2: learning to play monopoly like a pro üé©üè®üí∞ \n","<img src=\"https://miro.medium.com/max/960/0*T_ritMztZYRTR74R.jpg\" width=\"30%\">"],"metadata":{"id":"JyiplNB_Wqm9"}},{"cell_type":"markdown","source":["###**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n"],"metadata":{"id":"QXFCOeFRY4St"}},{"cell_type":"markdown","source":["What would the 4 RL ingredients be?\n","\n","* The **agent** is you, the one who wants to win at Monopoly.\n","* Your **actions** are the ones you see on this screenshot below:\n","<img src=\"https://miro.medium.com/max/1050/0*XVrxPYg9fKpAWtwj\" width=\"70%\">\n","*The **environment** is the current state of the game, including the *list of properties*, *positions*, and *cash amounts each player* has. \n","  - There is also the strategy of your opponent, which is something you cannot predict and lies outside of your control.\n","* And the **reward** is 0, except in your last move, where it is +1 if you win the game, and -1 if you go bankrupt. \n","  - This reward formulation makes sense but makes the problem hard to solve. \n","  - As we said above, a more sparse reward means a harder solution. Because of this, there are other ways to model the reward, making them noisier but less sparse."],"metadata":{"id":"kFJt8vq6X24V"}},{"cell_type":"markdown","source":["### **self-play**\n","When you play against another person in Monopoly, you do not know how she or he will play. \n","* What you can do is play against yourself. As you learn to play better, your opponent does too (because it is you), forcing you to level up your game to keep on winning. You see the positive feedback loop.\n","\n","This trick is called **self-play**. It gives us a path to bootstrap intelligence without using the external advice of an expert player.\n","\n","  * **Self-play** is the main difference between [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) and [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch), the two models developed by DeepMind that play the game of Go better than any human."],"metadata":{"id":"-WBm2F7rZGoi"}}]}