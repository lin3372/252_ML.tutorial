{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Hands-on RL_From zero to hero, step by step-220713.ipynb","provenance":[],"authorship_tag":"ABX9TyO5kyLBc/1TfJmYW1IYvSCJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[Hands-On Reinforcement Learning Course: Part 1 - From zero to hero, step by step](https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-1-269b50e39d08)\n","\n","by [Pau Labarta Bajo](https://pau-labarta-bajo.medium.com/?source=post_page-----269b50e39d08--------------------------------), Nov 27, 2021.  [[github - reinforcement_learning_training_loop.py]](https://gist.github.com/Paulescu/9a2270e4801162f1dbb3e183716c4fc5/raw/2a49f76707b52e9920f656ed771292e1dfa9e276/reinforcement_learning_training_loop.py)\n","\n","Summarized and Revised by Ivan H.P. Lin, July 13 2022"],"metadata":{"id":"Mi3Gntqqhw_t"}},{"cell_type":"markdown","source":["#0 - Contents\n","1. What is a Reinforcement Learning problem? ü§î\n","2. Policies üëÆüèΩ and value functions.\n","3. How to generate the training data? üìä\n","4. Python boilerplate code.üêç\n","5. Recap ‚ú®\n","6. Homework üìö\n","7. What‚Äôs next? ‚ù§Ô∏è\n","\n","Let‚Äôs start!"],"metadata":{"id":"krYlwHGuheqF"}},{"cell_type":"markdown","source":["#1 - What is a reinforcement learning problem? ü§î\n","**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","<figure><center>\n","<img src=\"https://miro.medium.com/max/850/1*w-BQXtdOQgzdZbcZ8yaiCQ.jpeg\" width=\"60%\" >\n","<figcaption>Reinforcement learning ingredients (Image by the author) </figcaption> \n","</center></figure>"],"metadata":{"id":"0yxeIlc4iPq1"}},{"cell_type":"markdown","source":["An intelligent **agent** ü§ñ needs to learn, through trial and error, how to take **actions** inside and **environment** üåé in order to maximize a **cumulative reward**.\n","\n","If you are asking yourself these questions you are on the right track.\n","\n","The definition I just gave introduces a bunch of terms that you might not be familiar with. In fact, they are ambiguous on purpose. This generality is what makes RL applicable to a wide range of seemingly different learning problems. This is the philosophy behind mathematical modeling, which stays at the roots of RL.\n","\n","Let's use few examples to explain **AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**:"],"metadata":{"id":"VKMFBfasif_t"}},{"cell_type":"markdown","source":["##**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","### Example 1: learning to walk üö∂üèΩ‚Äç‚ôÄÔ∏èüö∂üèøüö∂‚Äç‚ôÄÔ∏è"],"metadata":{"id":"aUyIV7F2VE5s"}},{"cell_type":"markdown","source":["### **AGENT**\n","* The **agent** is my son, Kai. And he wants to stand up and walk. His muscles are strong enough at this point in time to have a chance at it. The learning problem for him is: how to sequentially adjust his body position, including several angles on his legs, waist, back, and arms to balance his body and not fall.\n"],"metadata":{"id":"WH9gDsyCjD32"}},{"cell_type":"markdown","source":["### **ENVIRONMENT**\n","* The **environment** is the physical world surrounding him, including the laws of physics. The most important of which is gravity. Without gravity the learning-to-walk problem would drastically change, and even become irrelevant: why would you wanna walk in a world where you can simply fly? Another important law in this learning problem is Newton‚Äôs third law, which in plain words tells that if you fall on the floor, the floor is going to hit you back with the same strength. Ouch!"],"metadata":{"id":"LCc4f4_jjdZ1"}},{"cell_type":"markdown","source":["### **ACTION**\n","* The **actions** are all the updates in these body angles, which determine his body position and speed as he starts chasing things around. \n","  - Sure he can do other things at the same time, like imitating the sound of a cow, but these are probably not helping him accomplish his goal. We ignore these actions in our framework. Adding unnecessary actions does not change the modeing step, but it makes the problem harder to solve later on."],"metadata":{"id":"_uLvFVjDSVSU"}},{"cell_type":"markdown","source":["### **REWARDS**, and ***Cumulative Reward***\n","* The **reward** he receives is a stimulus coming from the brain, that makes him happy or makes him feel pain. There is the negative reward he experiences when falling on the floor, which is physical pain maybe followed by frustration. \n","  - On the other side, there are several things that contribute positively to his happiness, like the happiness of getting to places faster üë∂üöÄ, or the external stimulus that comes from my wife Jagoda and I when we say ‚ÄúGood job!‚Äù or ‚ÄúBravo!‚Äù to each attempt and marginal improvement he shows."],"metadata":{"id":"8RLPytNVSgZr"}},{"cell_type":"markdown","source":["* A little bit more about rewards üí∞\n","Some actions might seem very appealing for the baby at the beginning, like trying to run to get a boost of excitement. However, he soon learns that in some (or most) cases he ends up falling on his face, and experiencing an extended period of pain and tears. This is why **intelligent agents maximize cumulative reward**, and **not marginal reward**. \n","  * **They trade short-term rewards with long-term ones. An action that would give immediate reward, but put my body in a position about to fall, is not an optimal one.**\n","\n","* The frequency and intensity of the rewards are key for helping the agent learn. Very infrequent (sparse) feedback means harder learning. Think about it, if you do not know if what you do is good or bad, how can you learn? This is one of the main reasons why some RL problems are harder than others.\n","\n","* Reward shaping is a tough modeling decision for many real-world RL problems.\n"],"metadata":{"id":"dwc6dmA9SigL"}},{"cell_type":"markdown","source":["##**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","### Example 2: learning to play monopoly like a pro üé©üè®üí∞ \n","<img src=\"https://miro.medium.com/max/960/0*T_ritMztZYRTR74R.jpg\" width=\"30%\">"],"metadata":{"id":"JyiplNB_Wqm9"}},{"cell_type":"markdown","source":["###**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n"],"metadata":{"id":"QXFCOeFRY4St"}},{"cell_type":"markdown","source":["What would the 4 RL ingredients be?\n","\n","* The **agent** is you, the one who wants to win at Monopoly.\n","* Your **actions** are the ones you see on this screenshot below:\n","<img src=\"https://miro.medium.com/max/1050/0*XVrxPYg9fKpAWtwj\" width=\"70%\">\n","*The **environment** is the current state of the game, including the *list of properties*, *positions*, and *cash amounts each player* has. \n","  - There is also the strategy of your opponent, which is something you cannot predict and lies outside of your control.\n","* And the **reward** is 0, except in your last move, where it is +1 if you win the game, and -1 if you go bankrupt. \n","  - This reward formulation makes sense but makes the problem hard to solve. \n","  - As we said above, a more sparse reward means a harder solution. Because of this, there are other ways to model the reward, making them noisier but less sparse."],"metadata":{"id":"kFJt8vq6X24V"}},{"cell_type":"markdown","source":["### **self-play**\n","When you play against another person in Monopoly, you do not know how she or he will play. \n","* What you can do is play against yourself. As you learn to play better, your opponent does too (because it is you), forcing you to level up your game to keep on winning. You see the positive feedback loop.\n","\n","This trick is called **self-play**. It gives us a path to bootstrap intelligence without using the external advice of an expert player.\n","\n","  * **Self-play** is the main difference between [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) and [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch), the two models developed by DeepMind that play the game of Go better than any human."],"metadata":{"id":"-WBm2F7rZGoi"}},{"cell_type":"markdown","source":["##**AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**\n","### Example 3: learning to drive üöó "],"metadata":{"id":"UeFqcdrNcHkT"}},{"cell_type":"markdown","source":["Learning to drive a car is not easy. The goal of the driver is clear: to get from point A to point B, comfortably for her and any passengers on board.\n","\n","There are many external aspects to the driver that make driving challenging, including:\n","\n","  - other drivers behavior\n","  - traffic signs\n","  - pedestrian behaviors\n","  - pavement conditions\n","  - weather conditions.\n","  - ‚Ä¶ even fuel optimization (who wants to spend extra on this?)\n","\n","How would we approach this problem with reinforcement learning?"],"metadata":{"id":"64OY_92pcT7q"}},{"cell_type":"markdown","source":["### **AGENT**, **ENVIRONMENT**, **ACTION**, and **REWARDS**"],"metadata":{"id":"qtXc4cVZcxAK"}},{"cell_type":"markdown","source":["* The **agent** is the driver who wants to get from A to B, comfortably.\n","* The **state of the environment** the driver observes has lots of things, including the position, speed and acceleration of the car, all other cars, passengers, road conditions or traffic signs. Transforming such a big vector of inputs into an appropriate action is challenging as you can imagine.\n","* The **actions** are basically three: the direction of the steering wheel, throttle intensity and break intensity.\n","* The **reward** after each action is a weighted sum of the different aspects you need to balance when driving. A decrease in distance to point B brings a positive reward, while an increase a negative one. To ensure no collisions, getting too close (or even colliding) with another car, or even a pedestrian should have a very big negative reward. Also, in order to encourage smooth driving, sharp changes in speed or direction contribute to a negative reward."],"metadata":{"id":"uFlBsuSbcfky"}},{"cell_type":"markdown","source":["## RL Diagram\n","After these 3 examples, I hope the following representation of RL elements and how they play together makes sense\n","\n","<figure><center>\n","<img src=\"https://miro.medium.com/max/850/1*w-BQXtdOQgzdZbcZ8yaiCQ.jpeg\" width=\"60%\" >\n","<figcaption>Reinforcement learning ingredients (Image by the author) </figcaption> \n","</center></figure>"],"metadata":{"id":"FffnVvGbd_BJ"}},{"cell_type":"markdown","source":["# 2. **Policies**, **Value functions**, **Bellman Equation**"],"metadata":{"id":"kUQUQN6WfJCY"}},{"cell_type":"markdown","source":["## **Policy**, \"- Deterministic / Stochastic\"\n","The agent picks the action she thinks is the best based on the current state of the environment.\n","\n","This is the agent‚Äôs strategy, commonly referred to as the agent‚Äôs **policy**.\n","\n","> A **policy** is a learned mapping from states to actions.\n","- **Solving a reinforcement learning probem means finding the best possible policy.**"],"metadata":{"id":"pSI63hiFfW3g"}},{"cell_type":"markdown","source":["### **Deterministic** or **Stochastic**, *policy optimization methods*\n","Policies are either **deterministic**, when they map each state to one action,\n","**$$\\pi(state) = action$$**\n","\n","or **stochastic** when they map each state to a probability distribution over all possible actions.\n","\n","**$$\\pi(state) = (prob(action_1), prob(action_2),\\dots ,prob(action_N))$$**"],"metadata":{"id":"dzB_6UG3f-Z3"}},{"cell_type":"markdown","source":["**Stochastic** is a word you often read and hear in Machine Learning and it essentially means uncertain, random. In environments with high uncertainty, like Monopoly where you are rolling dices, **stochastic** policies are better than deterministic ones.\n","\n","There exist several methods to actually compute this optimal policy. These are called **policy optimization method**s."],"metadata":{"id":"wEBS6br6iTdG"}},{"cell_type":"markdown","source":["## **Value functions**, ***q*-value functions**\n","Sometimes, depending on the problem, instead of directly trying to find the optimal policy, one can try to find the **value function** associated with that optimal policy.\n","\n","But, what is a value function?  And what does value mean in this context?"],"metadata":{"id":"gnCSkx3x_cCh"}},{"cell_type":"markdown","source":["The **value** is a number associated with each state **$s$** of the environment that estimates how good it is for the agent to be in state **$s$**.\n","\n","It is the cumulative reward the agent collects when starting at state **$s$** and choosing actions according to policy **$œÄ$** .\n","\n","**A value function is a learned mapping from states to values**.\n","The value function of a policy is commonly denoted as \n","\n","* **$v_\\pi(s) =$ cumulative reward when the agent starts at state $s$ and follow policy $œÄ$**"],"metadata":{"id":"zFWetGYOADqW"}},{"cell_type":"markdown","source":["###**Bellman equation**, ***Q*-learning**\n","Value functions can also map pairs of **(action, state)** to values. In this case, they are called ***q-value*** functions.\n","\n","* $q_œÄ(s)$=cumulative reward when the agent starts at state **$s$**, take an action **$a$**,  and follow policy **$œÄ$** thereafter\n","\n","The optimal value function (or $q$-value function) satisfies a mathematical equation, called the **Bellman equation**.\n","<figure><center>\n","<img src=\"https://miro.medium.com/max/1400/0*8ltyj-zO-6Li3zuX.png\" width=\"50%\">\n","</figure></center>\n","\n","This equation is useful because it can be transformed into an iterative procedure to find the optimal value function\n","\n","* why are value functions useful?\n","  - Because you can infer an optimal policy from an optimal q-value function.\n","* How?  \n","  - The optimal policy is the one where at each state **$s$** the agent chooses the action **$a$** that maximizes the ***$q$***-value function\n","* So, you can jump from optimal policies to optimal q-functions, and vice versa üòé.\n","\n","There are several RL algorithms that focus on finding optimal ***q***-value functions. These are called **Q-learning methods**  \n","\n"],"metadata":{"id":"_EQmPmfMB0gF"}},{"cell_type":"markdown","source":["#3. [OpenAI's Gym](https://gym.openai.com/), [DeepMind's MoJoCo](https://www.endtoend.ai/envs/gym/mujoco/) - How to generate training data? üìä\n","Reinforcement learning agents are VERY data-hungry"],"metadata":{"id":"iOsSdEUMREIp"}},{"cell_type":"markdown","source":["A way to overcome this hurdle is by using **simulated environments**. Writing the engine that simulates the environment usually requires more work than solving the RL problem. Also, changes between different engine implementations can render comparisons between algorithms meaningless.\n","\n","This is why guys at **OpenAI** released the [Gym toolkit](https://gym.openai.com/) back in 2016. OpenAIs‚Äôs gym offers a standardized API for a collection of environments for different problems, including\n","\n","- the classic Atari games,\n","- robotic arms\n","- or landing on the Moon (well, a simplified one)\n","\n","**OpenAI Gym** also defines a standard API to build environments, allowing third parties (like you) to create and make your environments available to others.\n","\n","There are proprietary environments too, like **[MuJoCo](https://www.endtoend.ai/envs/gym/mujoco/)** (recently bought by DeepMind). MuJoCo is an environment where you can solve continuous control tasks in 3D, like learning to walk üë∂.\n","\n","If you are interested in **self-driving cars**, then you should check out [CARLA](https://carla.org/), the most popular open urban driving simulator."],"metadata":{"id":"zxDraJwQqwl4"}},{"cell_type":"markdown","source":["# 4. Python boilerplate code üêç\n","Pseudo sample code"],"metadata":{"id":"Uz5FpdmzsSr-"}},{"cell_type":"markdown","source":["\n","\n","```\n","import random\n","\n","def train(n_episodes: int):\n","    \"\"\"\n","    Pseudo-code of a Reinforcement Learning agent training loop\n","    \"\"\"\n","\n","    # python object that wraps all environment logic. Typically you will\n","    # be using OpenAI gym here.\n","    env = load_env()\n","\n","    # python object that wraps all agent policy (or value function)\n","    # parameters, and action generation methods.\n","    agent = get_rl_agent()\n","\n","    for episode in range(0, n_episodes):\n","\n","        # random start of the environmnet\n","        state = env.reset()\n","\n","        # epsilon is parameter that controls the exploitation-exploration trade-off.\n","        # it is good practice to set a decaying value for epsilon\n","        epsilon = get_epsilon(episode)\n","\n","        done = False\n","        while not done:\n","\n","            if random.uniform(0, 1) < epsilon:\n","                # Explore action space\n","                action = env.action_space.sample()\n","            else:\n","                # Exploit learned values (or policy)\n","                action = agent.get_best_action(state)\n","\n","            # environment transitions to next state and maybe rewards the agent.\n","            next_state, reward, done, info = env.step(action)\n","\n","            # adjust agent parameters. We will see how later in the course.\n","            agent.update_parameters(state, action, reward, next_state)\n","\n","            state = next_state\n","```\n","\n"],"metadata":{"id":"vPORkHP0r3-u"}},{"cell_type":"markdown","source":["## **$œµ$** and **exploration-exploitation**"],"metadata":{"id":"szQyH-2PskAe"}},{"cell_type":"markdown","source":["Epsilon (**$œµ$**) is a key parameter to ensure our agent explores the environment enough, before drawing definite conclusions on what is the best action to take in each state.\n","\n","It is a value between 0 and 1, and it represents the probability the agent chooses a random action instead of what she thinks is the best one.\n","\n","This tradeoff between *exploring new strategies* vs *sticking to already known ones* is called the **exploration-exploitation problem**. This is a key ingredient in RL problems and something that distinguishes RL problems from supervised machine learning.\n","\n","Technically speaking, we want the agent to find the global optimum, not a local one.\n","\n","It is good practice to start your training with a large value (e.g. 50%) and progressively decrease after each episode. This way the agent explores a lot at the beginning, and less as she perfects her strategy."],"metadata":{"id":"u_xv8l8zs15t"}},{"cell_type":"markdown","source":["# 5. Recap"],"metadata":{"id":"rlnmtYxttigw"}},{"cell_type":"markdown","source":["* Every RL problem has an agent (or agents), environment, actions, states and rewards.\n","* The agent sequentially takes actions with the goal of maximizing total rewards. For that she needs to find the optimal policy.\n","* Value functions are useful as they give us an alternative path to find the optimal policy.\n","* In practice, you need to try different RL algorithms for your problem, and see what works best.\n","* RL agents need a lot of training data to learn. OpenAI gym is a great tool to re-use and create your environments.\n","* Exploration vs exploitation is necessary when training RL agents, to ensure the agent does not get stuck in local optimums."],"metadata":{"id":"9z4yKJDStmmt"}}]}