{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Understanding PyTorch with an example: a step-by-step tutorial.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyODGWjPznfXI6iQkk0oExoe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YXCA0NFpsYw2","executionInfo":{"status":"ok","timestamp":1652588651922,"user_tz":-480,"elapsed":1108,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# [Understanding PyTorch with an example: a step-by-step tutorial](https://medium.com/towards-data-science/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)\n","\n","- By [Daniel Godoy](https://medium.com/@dvgodoy), May 7, 2019, 21 min read\n","- From [medium](https://medium.com/towards-data-science/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)\n","- Tailored customized for Ivan HP Lin, (WIP - Pytoch - autograd)\n"],"metadata":{"id":"dmrXRHcJsclV"}},{"cell_type":"markdown","source":["PyTorch is also very pythonic, meaning, it feels more natural to use it if you already are a Python developer.\n","\n","Besides, using PyTorch may even improve your health, according to [Andrej Karpathy](https://twitter.com/karpathy/status/868178954032513024) :-)\n","\n","**PyTorch** is the **fastest growing** Deep Learning framework and it is also used by **Fast.ai** in its MOOC, [Deep Learning for Coders](https://course.fast.ai/) and its [library](https://docs.fast.ai/)\n","\n","Since this is quite a **long post**, I built a Table of Contents to make navigation easier, should you use it as a **mini-course** and work your way through the content one topic at a time\n","\n","* Table of Contents\n","  - A Simple Regression Problem\n","  - Gradient Descent\n","  - Linear Regression in Numpy\n","  - PyTorch\n","  - Autograd\n","  - Dynamic Computation Graph\n","  - Optimizer\n","  - Loss\n","  - Model\n","  - Dataset\n","  - DataLoader\n","  - Evaluation"],"metadata":{"id":"XxQv4g7FtLxe"}},{"cell_type":"markdown","source":["## A Simple Regression Problem\n","\n","In this tutorial, I will stick with a simple and familiar problem: a linear regression with a single feature $x$! It doesn’t get much simpler than that\n","<figure>\n","<center>\n","<img src=\"https://miro.medium.com/max/282/1*a7_GUQQT5BjvAhh3qq0JwA.png\" width=\"35%\">\n","<figcaption>Simple Linear Regression model</figcaption></center>\n","</figure>"],"metadata":{"id":"1fTDKMDNvDAm"}},{"cell_type":"markdown","source":["### Data Generation\n","Let’s start generating some **synthetic data**: we start with a vector of 100 points for our **feature** $\\bf x$ and create our **labels** using $a = 1, b = 2$ and some Gaussian noise $ϵ$.\n","\n","Next, let’s **split** our synthetic data into **train** and **validation** sets, shuffling the array of indices and using the first 80 shuffled points for training."],"metadata":{"id":"_jQX3znzxxOl"}},{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot\n","\n","# Data Generation\n","np.random.seed(42)\n","x = np.random.rand(100, 1)\n","y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n","\n","# Shuffles the indices\n","idx = np.arange(100)\n","np.random.shuffle(idx)\n","\n","# Uses first 80 random indices for train\n","train_idx = idx[:80]\n","# Uses the remaining indices for validation\n","val_idx = idx[80:]\n","\n","# Generates train and validation sets\n","x_train, y_train = x[train_idx], y[train_idx]\n","x_val, y_val = x[val_idx], y[val_idx]"],"metadata":{"id":"p5apNSmkyae9","executionInfo":{"status":"ok","timestamp":1652588651923,"user_tz":-480,"elapsed":9,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["f, (ax1, ax2) = pyplot.subplots(1, 2, figsize=(12,8)) \n","\n","ax1.set_title('Generated Data - Train')\n","ax1.set_xlabel(f'$x$')\n","ax1.set_ylabel(f'$y$')\n","ax1.set_xlim(0, 1.1)\n","ax1.set_ylim(0.9, 3.1)\n","ax1.set_aspect(0.4)\n","ax1.scatter(x_train, y_train, c='g', marker='.')\n","\n","ax2.set_title('Generated Data - Validation')\n","ax2.set_xlabel(f'$x$')\n","ax2.set_ylabel(f'$y$')\n","ax2.set_xlim(0, 1.1)\n","ax2.set_ylim(0.9, 3.1)\n","ax2.set_aspect(0.4)\n","ax2.scatter(x_val, y_val, c='r', marker='x')\n","\n","pyplot.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"kp0GHczF0BsU","executionInfo":{"status":"ok","timestamp":1652588651923,"user_tz":-480,"elapsed":8,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"4c0f279d-8c1c-4770-8692-3e329c77a12b"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x576 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["We know that $a = 1$ and $b = 24$, but now let’s see how close we can get to the true values by using **gradient descent** and the 80 points in the **training set**…"],"metadata":{"id":"jFYTResf420y"}},{"cell_type":"markdown","source":["## Gradient Descent"],"metadata":{"id":"abGvSEjjvPIV"}},{"cell_type":"markdown","source":["### Step 1: Compute the Loss\n","\n","For a regression problem, the loss is given by the **Mean Square Error (MSE)**, that is, the average of all squared differences between **labels** ($y$) and **predictions** ($a + bx$)."],"metadata":{"id":"PLPzVwKrFcvJ"}},{"cell_type":"markdown","source":["It is worth mentioning that, \n","- if we use **all points** in the training set ($N$) to compute the loss, we are performing a **batch** gradient descent. \n","- If we were to use a single point at each time, it would be a **stochastic** gradient descent. \n","- Anything else (n) **in-between 1 and N** characterizes a **mini-batch** gradient descent\n","\n","<figure>\n","<center>\n","<img src=\"https://miro.medium.com/max/518/1*7fmJUcQT578OBfX7Q8hluQ.png\" width=\"50%\">\n","<figcaption>Loss: Mean Squared Error (MSE)</figcaption></center>"],"metadata":{"id":"kSV5j-PmFxhv"}},{"cell_type":"markdown","source":["### Step 2: Compute the Gradients\n","\n","<figure>\n","<center>\n","<img src=\"https://miro.medium.com/max/1050/1*YvTj1B-h1gzSI5F24OgrrA.png\" width=\"70%\">\n","<figcaption>Computing gradients w.r.t coefficients $a$ and $b$</figcaption></center>"],"metadata":{"id":"kwABCedlGlNA"}},{"cell_type":"markdown","source":["### Step 3: Update the Parameters\n","\n","In the final step, we use the **gradients** to update the parameters. Since we are trying to **minimize** our **losses**, we **reverse the sign** of the gradient for the update.\n","\n","There is still another parameter to consider: the **learning rate**, denoted by the Greek letter eta $η$ (that looks like the letter n), which is the multiplicative factor that we need to apply to the gradient for the parameter update.\n","\n","<figure>\n","<center>\n","<img src=\"https://miro.medium.com/max/314/1*eWnUloBYcSNPRBzVcaIr1g.png\" width=\"30%\">\n","<figcaption>Updating coefficients $a$ and $b$ using computed gradients and a learning rate</figcaption></center>"],"metadata":{"id":"tIMyZRNjHEne"}},{"cell_type":"markdown","source":["### Step 4: Rinse and Repeat!\n","\n","Now we use the updated parameters to go back to Step 1 and restart the process.\n","\n","* An **epoch is complete whenever every point has been already used for computing the loss**. \n","  - For **batch** gradient descent, this is trivial, as it uses all points for computing the loss — one epoch is the same as one update. \n","  - For **stochastic** gradient descent, one epoch means $N$ updates, \n","  - For **mini-batch** (of size $n$), one epoch has ${N} \\over {n}$ updates."],"metadata":{"id":"Ro_eJbBZHxyr"}},{"cell_type":"markdown","source":["## Linear Regression in Numpy"],"metadata":{"id":"IOZmm9cuvRls"}},{"cell_type":"markdown","source":["For training a model, there are **two initialization steps**:\n","\n","* Random initialization of parameters/weights (we have only two, a and b) — lines 3 and 4;\n","* Initialization of hyper-parameters (in our case, only learning rate and number of epochs) — lines 9 and 11;\n","\n","Make sure to always initialize your random seed to ensure reproducibility of your results. As usual, the random seed is 42, the least random of all random seeds one could possibly choose :-)\n","\n","\n","For each epoch, there are four training steps:\n","1. Compute model’s predictions — this is the **forward pass** — line 15;\n","2. Compute the loss, using predictions and and labels and the appropriate **loss function** for the task at hand — lines 18 and 20;\n","3. Compute the **gradients** for every parameter — lines 23 and 24;\n","4. **Update the parameters** — lines 27 and 28;\n","\n","Just keep in mind that, if you don’t use batch gradient descent (our example does),you’ll have to write an **inner loop** to perform the **four training steps** for either each **individual point (stochastic)** or **$n$ points (mini-batch)**. We’ll see a mini-batch example later down the line."],"metadata":{"id":"EYk1akvkI-Iu"}},{"cell_type":"code","source":["import numpy as np\n","# Initializes parameters \"a\" and \"b\" randomly\n","np.random.seed(42)\n","a = np.random.randn(1)\n","b = np.random.randn(1)\n","\n","print(f'a and b at initialization: {a}, {b}]')\n","\n","# Sets learning rate\n","lr = 1e-1\n","# Defines number of epochs\n","n_epochs = 1000\n","\n","for epoch in range(n_epochs):\n","    # Computes our model's predicted output\n","    yhat = a + b * x_train\n","    \n","    # How wrong is our model? That's the error! \n","    error = (y_train - yhat)\n","    # It is a regression, so it computes mean squared error (MSE)\n","    loss = (error ** 2).mean()\n","    \n","    # Computes gradients for both \"a\" and \"b\" parameters\n","    a_grad = -2 * error.mean()\n","    b_grad = -2 * (x_train * error).mean()\n","    \n","    # Updates parameters using gradients and the learning rate\n","    a = a - lr * a_grad\n","    b = b - lr * b_grad\n","    \n","print(f'a and b after our gradient descent: {a}, {b}]')\n","\n","# Sanity Check: do we get the same results as our gradient descent?\n","from sklearn.linear_model import LinearRegression\n","linr = LinearRegression()\n","linr.fit(x_train, y_train)\n","print(f'intercept and coef from Scikit-Learn: {linr.intercept_}, {linr.coef_[0]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDUFIxPVOo00","executionInfo":{"status":"ok","timestamp":1652588652756,"user_tz":-480,"elapsed":840,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"a2102b8f-b13b-4fe7-f89b-1a9ef394a98a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["a and b at initialization: [0.49671415], [-0.1382643]]\n","a and b after our gradient descent: [1.02354094], [1.96896411]]\n","intercept and coef from Scikit-Learn: [1.02354075], [1.96896447]\n"]}]},{"cell_type":"markdown","source":["## PyTorch"],"metadata":{"id":"o40sLAiIvXWs"}},{"cell_type":"markdown","source":["### Tensor\n","In Numpy, you may have an **array** that has **three dimensions**, right? That is, technically speaking, a **tensor**.\n","\n","A **scalar** (a single number) has **zero** dimensions, a **vector has one** dimension, a **matrix has two** dimensions and a **tensor has three or more** dimensions. That’s it!\n","\n","But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, **everything is either a scalar or a tensor**."],"metadata":{"id":"SPjLiuVMQLGt"}},{"cell_type":"markdown","source":["### Loading Data, Devices and CUDA\n","”*How do we go from Numpy’s arrays to PyTorch’s tensors*”, you ask? That’s what ***from_numpy*** is good for. It returns a **CPU tensor**, though.\n","\n","“But I want to use my fancy **GPU…”**, you say. No worries, that’s what ***to()*** is good for. It sends your tensor to whatever **device** you specify, including your **GPU** (referred to as **cuda** or **cuda:0**).\n","\n","“*What if I want my code to fallback to CPU if no GPU is available*?”, you may be wondering… PyTorch got your back once more — you can use ***cuda.is_available()*** to find out if you have a GPU at your disposal and set your device accordingly.\n","\n","You can also easily **cast** it to a lower precision (32-bit float) using ***float()***."],"metadata":{"id":"YJaE3S9hQpAF"}},{"cell_type":"code","source":["! pip install torchviz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iqe1bAVTRnIU","executionInfo":{"status":"ok","timestamp":1652588657493,"user_tz":-480,"elapsed":4740,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"3c84bad3-2787-4248-9c53-f6ca2bb24b3a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=f545cf22b7216b2208616eac6d630f3dfb97aecd6b09b6af6004ca6456989be3\n","  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torchviz import make_dot\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n","# and then we send them to the chosen device\n","x_train_tensor = torch.from_numpy(x_train).float().to(device)\n","y_train_tensor = torch.from_numpy(y_train).float().to(device)\n","\n","# Here we can see the difference - notice that .type() is more useful\n","# since it also tells us WHERE the tensor is (device)\n","print(f'device={device}')\n","print(type(x_train), type(x_train_tensor), x_train_tensor.type())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HA_xPAlfRWrr","executionInfo":{"status":"ok","timestamp":1652588659704,"user_tz":-480,"elapsed":2217,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"fa432af6-7d5b-4442-b39c-d2fc8b94e838"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["device=cpu\n","<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"]}]},{"cell_type":"markdown","source":["If you compare the types of both variables, you’ll get what you’d expect: **numpy.ndarray** for the first one and **torch.Tensor** for the second one.\n","\n","But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s type(), it will reveal its location — **torch.cuda.FloatTensor** — a GPU tensor in this case."],"metadata":{"id":"o2LoXVMYTuXi"}},{"cell_type":"markdown","source":["### Creating Parameters\n","\n","What distinguishes a tensor used for data — like the ones we’ve just created — from a **tensor** used as a (***trainable***)** parameter/weight**?\n","\n","The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters’ values, that is). That’s what the ***requires_grad=True*** argument is good for. It tells PyTorch we want it to compute gradients for us.\n","\n","You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right? Not so fast…"],"metadata":{"id":"jPW9H4u09X6J"}},{"cell_type":"code","source":["# FIRST\n","# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n","# since we want to apply gradient descent on these parameters, we need\n","# to set REQUIRES_GRAD = TRUE\n","a = torch.randn(1, requires_grad=True, dtype=torch.float)\n","b = torch.randn(1, requires_grad=True, dtype=torch.float)\n","print(a, b)\n","\n","# SECOND\n","# But what if we want to run it on a GPU? We could just send them to device, right?\n","a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n","b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n","print(a, b)\n","# Sorry, but NO! The to(device) \"shadows\" the gradient...\n","\n","# THIRD\n","# We can either create regular tensors and send them to the device (as we did with our data)\n","a = torch.randn(1, dtype=torch.float).to(device)\n","b = torch.randn(1, dtype=torch.float).to(device)\n","# and THEN set them as requiring gradients...\n","a.requires_grad_()\n","b.requires_grad_()\n","print(a, b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fCCo21L9taD","executionInfo":{"status":"ok","timestamp":1652588924337,"user_tz":-480,"elapsed":338,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"3bf2fdd6-4bb6-402a-ff66-a6b7a5abefdf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.8461], requires_grad=True) tensor([-0.5389], requires_grad=True)\n","tensor([-0.2595], requires_grad=True) tensor([0.7324], requires_grad=True)\n","tensor([-0.1909], requires_grad=True) tensor([-0.8241], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["The first chunk of code creates two nice tensors for our parameters, gradients and all. But they are CPU tensors.\n","```\n","# FIRST\n","tensor([-0.5531], requires_grad=True)\n","tensor([-0.7314], requires_grad=True)\n","```\n","\n","In the second chunk of code, we tried the naive approach of sending them to our GPU. We succeeded in sending them to another device, but we ”lost” the gradients somehow…\n","```\n","# SECOND\n","tensor([0.5158], device='cuda:0', grad_fn=<CopyBackwards>) tensor([0.0246], device='cuda:0', grad_fn=<CopyBackwards>)\n","```\n","\n","In the third chunk, we first send our tensors to the device and then use requires_grad_() method to set its requires_grad to True in place.\n","\n","```\n","# THIRD\n","tensor([-0.8915], device='cuda:0', requires_grad=True) tensor([0.3616], device='cuda:0', requires_grad=True)\n","```"],"metadata":{"id":"eHXQYhzL-mQa"}},{"cell_type":"markdown","source":["In PyTorch, every method that ends with an ***underscore (_)*** makes **changes in-place**, meaning, they will **modify** the underlying variable.\n","\n","Although the last approach worked fine, it is much better to assign tensors to a device at the moment of their creation."],"metadata":{"id":"tb9a7t27Ja32"}},{"cell_type":"code","source":["# We can specify the device at the moment of creation - RECOMMENDED!\n","torch.manual_seed(42)\n","a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n","b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n","print(a, b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xerlqcLKJ10k","executionInfo":{"status":"ok","timestamp":1652591913636,"user_tz":-480,"elapsed":297,"user":{"displayName":"HP Ivan Lin","userId":"15345680110015535826"}},"outputId":"e75a885b-7a2d-4d51-d85e-965845baa2ba"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["## Dynamic Computation Graph"],"metadata":{"id":"wnjPIBnjvk-U"}},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{"id":"YViD9cm5vow8"}},{"cell_type":"markdown","source":["## Loss"],"metadata":{"id":"8eI2ukzcvsf8"}},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"-kzmyk-gvvx8"}},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"ba18H0wNvybb"}},{"cell_type":"markdown","source":["## DataLoader"],"metadata":{"id":"YMdDoYrwv1Fb"}},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"uwTfyuiAv3w7"}}]}